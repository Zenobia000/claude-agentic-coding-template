import logging
from datetime import datetime
from contextlib import asynccontextmanager
import time

from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from starlette.responses import StreamingResponse

# ä¾è³´æ³¨å…¥å’Œæœå‹™æ¨¡çµ„
from config import settings
from schemas import (
    QueryRequest, QueryResponse, HealthCheckResponse,
    ChatCompletionRequest, ChatCompletionResponse, ChatMessage, ChatCompletionChoice,
    ChatCompletionChunk, ChatCompletionChunkChoice
)
from rag_service import RAGService
from model_manager import model_manager

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- å…¨å±€è®Šæ•¸ ---
lifespan_context = {}

# --- FastAPI ç”Ÿå‘½é€±æœŸäº‹ä»¶ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPIæ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†
    """
    logger.info("ğŸš€ å•Ÿå‹•RAG APIæœå‹™... (Auto-reload enabled)")
    
    if not settings.OPENAI_API_KEY:
        logger.error("âŒ åš´é‡éŒ¯èª¤: OPENAI_API_KEYç’°å¢ƒè®Šæ•¸æœªè¨­å®šã€‚")
    
    rag_service = RAGService(settings=settings)
    # Run in background
    import asyncio
    asyncio.create_task(rag_service.init_rag())
    
    lifespan_context["rag_service"] = rag_service
    
    logger.info("ğŸ“– APIæ–‡æª”å¯åœ¨ /docs æŸ¥é–±")
    
    yield
    
    logger.info(" gracefully shutting down...")
    lifespan_context.clear()

# --- ä¾è³´æ³¨å…¥ ---
def get_rag_service() -> RAGService:
    """
    ä¾è³´æ³¨å…¥å‡½æ•¸ï¼Œç²å–RAGæœå‹™å¯¦ä¾‹ã€‚
    """
    service = lifespan_context.get("rag_service")
    if not service:
        raise HTTPException(status_code=503, detail="RAGæœå‹™å°šæœªåˆå§‹åŒ–ã€‚ à¦¸à¦¨")
    return service

# --- FastAPIæ‡‰ç”¨å¯¦ä¾‹ ---
app = FastAPI(
    title="OpenAIç›¸å®¹çš„RAG APIæœå‹™",
    description="ä¸€å€‹ç‚ºOpen-WebUIæä¾›æ”¯æ´çš„ã€ç›¸å®¹OpenAI APIçš„RAGæœå‹™",
    version="3.0.0",
    lifespan=lifespan
)

# --- ä¸­é–“ä»¶ ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API ç«¯é» ---

# --- OpenAIç›¸å®¹ç«¯é» ---
@app.get("/v1/models")
async def list_models():
    """
    è¿”å›æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ã€‚
    
    æ”¯æŒå¤šå€‹æä¾›å•†ï¼ˆOpenAIã€Anthropicã€Googleï¼‰çš„æ¨¡å‹ã€‚
    æ¯å€‹å°è©±æ¨¡å‹éƒ½æœ‰å…©å€‹ç‰ˆæœ¬ï¼š
    - åŸå§‹æ¨¡å‹ ID: ç´” LLM å°è©±ï¼ˆä¸ä½¿ç”¨ RAGï¼‰
    - {model_id}-rag: RAG æ¨¡å¼ï¼ˆæª¢ç´¢æ–‡æª”å¾Œå›ç­”ï¼‰
    """
    return {
        "object": "list",
        "data": model_manager.get_models_list()
    }

@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    rag_service: RAGService = Depends(get_rag_service)
):
    """
    OpenAI å…¼å®¹çš„ chat completions APIã€‚
    
    æ”¯æ´å¤šç¨®æ¨¡å‹å’Œæ¨¡å¼ï¼š
    - ä½¿ç”¨æ¨¡å‹ ID ä¸å¸¶ -rag å¾Œç¶´ï¼šç´” LLM å°è©±
    - ä½¿ç”¨æ¨¡å‹ ID å¸¶ -rag å¾Œç¶´ï¼šRAG æ¨¡å¼ï¼ˆæª¢ç´¢æ–‡æª”å¾Œå›ç­”ï¼‰
    
    æ”¯æŒçš„æä¾›å•†ï¼šOpenAI, Anthropic, Google
    """
    # ä½¿ç”¨ model_manager åˆ¤æ–·æ˜¯å¦ç‚º RAG æ¨¡å¼
    use_rag = model_manager.is_rag_model(request.model)
    base_model_id = model_manager.get_base_model_id(request.model)
    
    # RAG æ¨¡å¼éœ€è¦ç­‰å¾…åˆå§‹åŒ–å®Œæˆ
    if use_rag and not rag_service.prompt:
        raise HTTPException(status_code=503, detail="RAGæœå‹™æ­£åœ¨åˆå§‹åŒ–ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚ à¦¸à¦¨")
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not model_manager.get_model_info(request.model):
        raise HTTPException(
            status_code=400, 
            detail=f"æ¨¡å‹ '{request.model}' ä¸å­˜åœ¨ã€‚è«‹ä½¿ç”¨ GET /v1/models æŸ¥çœ‹å¯ç”¨æ¨¡å‹ã€‚"
        )
        
    last_user_message = next((msg.content for msg in reversed(request.messages) if msg.role == 'user'), None)
    
    if not last_user_message:
        raise HTTPException(status_code=400, detail="è«‹æ±‚ä¸­æ²’æœ‰ç”¨æˆ¶æ¶ˆæ¯ã€‚ à¦¸à¦¨")

    if request.stream:
        async def stream_generator():
            try:
                # æ ¹æ“šæ¨¡å¼é¸æ“‡è™•ç†æ–¹å¼
                if use_rag:
                    result = rag_service.query(last_user_message, model_id=base_model_id)
                else:
                    result = rag_service.chat(last_user_message, model_id=base_model_id)
                answer = result["answer"]
                
                # ç¬¬ä¸€å€‹å¡Šç™¼é€è§’è‰²
                chunk = ChatCompletionChunk(
                    model=request.model,
                    choices=[ChatCompletionChunkChoice(
                        index=0, 
                        delta=ChatMessage(role='assistant', content=''),
                        finish_reason=None
                    )]
                )
                yield f"data: {chunk.model_dump_json()}\n\n"
                
                # é€å­—ç™¼é€å…§å®¹
                for char in answer:
                    chunk = ChatCompletionChunk(
                        model=request.model,
                        choices=[ChatCompletionChunkChoice(
                            index=0, 
                            delta=ChatMessage(role='assistant', content=char),
                            finish_reason=None
                        )]
                    )
                    yield f"data: {chunk.model_dump_json()}\n\n"
                
                # ç™¼é€çµæŸå¡Š
                final_chunk = ChatCompletionChunk(
                    model=request.model,
                    choices=[ChatCompletionChunkChoice(
                        index=0,
                        delta=ChatMessage(role='assistant', content=''),
                        finish_reason='stop'
                    )]
                )
                yield f"data: {final_chunk.model_dump_json()}\n\n"
                
                # çµæŸæ¨™èªŒ
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                logger.error(f"Streaming error: {e}", exc_info=True)
                error_chunk = {"error": str(e)}
                yield f"data: {error_chunk}\n\n"
        
        return StreamingResponse(stream_generator(), media_type="text/event-stream")
    else:
        # æ ¹æ“šæ¨¡å¼é¸æ“‡è™•ç†æ–¹å¼
        if use_rag:
            result = rag_service.query(last_user_message, model_id=base_model_id)
        else:
            result = rag_service.chat(last_user_message, model_id=base_model_id)
        
        response_message = ChatMessage(role="assistant", content=result["answer"])
        choice = ChatCompletionChoice(index=0, message=response_message)
        
        return ChatCompletionResponse(model=request.model, choices=[choice])

# --- åŸæœ‰ç«¯é» ---
@app.get("/health", response_model=HealthCheckResponse, summary="å¥åº·æª¢æŸ¥")
async def health_check(rag_service: RAGService = Depends(get_rag_service)):
    is_rag_ready = rag_service.prompt is not None
    service_status = "healthy" if is_rag_ready else "degraded"
    
    return HealthCheckResponse(
        service="æ¨¡çµ„åŒ–RAG API",
        status=service_status,
        version=app.version,
        rag_ready=is_rag_ready,
        timestamp=datetime.now().isoformat()
    )

@app.post("/query", response_model=QueryResponse, summary="RAGå•ç­”ï¼ˆèˆŠç‰ˆï¼‰")
async def query_documents(
    request: QueryRequest,
    rag_service: RAGService = Depends(get_rag_service)
):
    if not rag_service.prompt:
        raise HTTPException(status_code=503, detail="RAGæœå‹™æ­£åœ¨åˆå§‹åŒ–ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚ à¦¸à¦¨")
    try:
        result = rag_service.query(request.question, request.top_k)
        return QueryResponse(**result)
    except Exception as e:
        logger.error(f"æŸ¥è©¢è™•ç†å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ä¼ºæœå™¨ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ã€‚ à¦¸à¦¨")
