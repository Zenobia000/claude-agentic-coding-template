import os
import logging
from typing import Dict, List, Optional
from datetime import datetime

from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from qdrant_client import QdrantClient
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

# å¾åŒç´šç›®éŒ„å°å…¥
from config import Settings
from model_manager import model_manager

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def elements_to_markdown(elements: List) -> str:
    """å°‡ unstructured å…ƒç´ è½‰æ›ç‚º Markdown æ ¼å¼"""
    markdown_lines = []
    for element in elements:
        elem_type = element.category
        text = element.text.strip()
        
        if not text:
            continue
        
        if elem_type == "Title":
            markdown_lines.append(f"## {text}\n")
        elif elem_type == "ListItem":
            markdown_lines.append(f"- {text}\n")
        else:
            markdown_lines.append(f"{text}\n")
    
    return "\n".join(markdown_lines)

class RAGService:
    def __init__(self, settings: Settings):
        self.settings = settings
        self.qa_chain = None
        self.retriever = None
        self.model_manager = model_manager

        # åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶
        self.embeddings = OpenAIEmbeddings(model=self.settings.EMBEDDING_MODEL)
        # é è¨­ LLM (ç”¨æ–¼å‘å¾Œå…¼å®¹)
        self.default_llm = None
        try:
            self.default_llm = model_manager.create_llm(
                self.settings.LLM_MODEL_NAME,
                self.settings.LLM_TEMPERATURE
            )
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•å‰µå»ºé è¨­ LLM: {e}")
        
        self.qdrant_client = QdrantClient(url=self.settings.QDRANT_URL)

        logger.info("âœ… RAG æœå‹™æ ¸å¿ƒçµ„ä»¶åˆå§‹åŒ–å®Œæˆ")

    async def init_rag(self) -> bool:
        """
        åˆå§‹åŒ–RAGç³»çµ±ï¼šæª¢æŸ¥ Qdrant ä¸­æ˜¯å¦æœ‰æ•¸æ“šä¸¦å»ºç«‹ QA éˆã€‚
        """
        try:
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collection_exists = self.qdrant_client.collection_exists(self.settings.QDRANT_COLLECTION_NAME)

            if not collection_exists:
                logger.warning(f"âŒ Qdrant é›†åˆ '{self.settings.QDRANT_COLLECTION_NAME}' ä¸å­˜åœ¨")
                logger.info("ğŸ’¡ è«‹å…ˆåœ¨ 04-embedding-application ä¸­å»ºç«‹å‘é‡ç´¢å¼•")
                return False

            # æª¢æŸ¥é›†åˆä¸­æ˜¯å¦æœ‰æ•¸æ“š
            collection_info = self.qdrant_client.get_collection(self.settings.QDRANT_COLLECTION_NAME)
            if collection_info.points_count == 0:
                logger.warning(f"âŒ Qdrant é›†åˆ '{self.settings.QDRANT_COLLECTION_NAME}' ä¸­æ²’æœ‰æ•¸æ“š")
                logger.info("ğŸ’¡ è«‹å…ˆåœ¨ 04-embedding-application ä¸­è¼‰å…¥æ–‡æª”æ•¸æ“š")
                return False

            logger.info(f"âœ… Qdrant é›†åˆ '{self.settings.QDRANT_COLLECTION_NAME}' å¯ç”¨ï¼ŒåŒ…å« {collection_info.points_count} å€‹å‘é‡")

            # å‰µå»ºQAéˆ
            template = """ä½ æ˜¯ä¸€å€‹å•ç­”åŠ©æ‰‹ã€‚æ ¹æ“šä»¥ä¸‹æª¢ç´¢åˆ°çš„æ–‡æª”å…§å®¹ä¾†å›ç­”å•é¡Œã€‚
å¦‚æœæª¢ç´¢åˆ°çš„æ–‡æª”ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚

Context: {context}

Question: {question}

Answer:"""

            self.prompt = ChatPromptTemplate.from_template(template)

            logger.info("âœ… RAGæœå‹™åˆå§‹åŒ–æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"RAGæœå‹™åˆå§‹åŒ–å¤±æ•—: {e}", exc_info=True)
            return False

    def query(self, question: str, model_id: Optional[str] = None, top_k: int = 3) -> Dict:
        """
        åŸ·è¡ŒRAGæŸ¥è©¢ï¼šå°‡æŸ¥è©¢å‘é‡åŒ–ï¼Œåœ¨ Qdrant ä¸­æœç´¢ï¼Œç”¨ LLM ç”Ÿæˆå›ç­”ã€‚
        
        Args:
            question: ç”¨æˆ¶å•é¡Œ
            model_id: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ IDï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é è¨­æ¨¡å‹
            top_k: æª¢ç´¢çš„æ–‡æª”æ•¸é‡
        """
        if not self.prompt:
            raise RuntimeError("RAGç³»çµ±æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡ŒæŸ¥è©¢ã€‚")

        try:
            # ç²å– LLM å¯¦ä¾‹
            llm = self._get_llm(model_id)
            
            # 1. å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡
            query_vector = self.embeddings.embed_query(question)
            logger.info(f"âœ… æŸ¥è©¢å‘é‡åŒ–å®Œæˆï¼Œç¶­åº¦: {len(query_vector)}")

            # 2. åœ¨ Qdrant ä¸­æœç´¢ç›¸ä¼¼å‘é‡
            search_results = self.qdrant_client.query_points(
                collection_name=self.settings.QDRANT_COLLECTION_NAME,
                query=query_vector,
                with_payload=True,
                limit=top_k
            ).points
            logger.info(f"âœ… æ‰¾åˆ° {len(search_results)} å€‹ç›¸é—œæ–‡æª”")

            # 3. æº–å‚™ä¸Šä¸‹æ–‡
            context_texts = []
            sources = []

            for hit in search_results:
                content = hit.payload.get('page_content', '')
                metadata = hit.payload.get('metadata', {})

                context_texts.append(content)
                sources.append({
                    "content": content[:500] + "..." if len(content) > 500 else content,
                    "metadata": metadata,
                    "score": hit.score
                })

            context = "\n\n".join(context_texts)

            # 4. ç”¨ LLM ç”Ÿæˆå›ç­”
            messages = self.prompt.format_messages(context=context, question=question)
            answer = llm.invoke(messages).content

            logger.info(f"âœ… RAG æŸ¥è©¢å®Œæˆ (model: {model_id or 'default'})")
            return {
                "question": question,
                "answer": answer,
                "sources": sources,
                "model": model_id or self.settings.LLM_MODEL_NAME,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"åŸ·è¡ŒæŸ¥è©¢æ™‚å‡ºéŒ¯: {e}", exc_info=True)
            raise RuntimeError(f"æŸ¥è©¢å¤±æ•—: {str(e)}")
    
    def chat(self, question: str, model_id: Optional[str] = None) -> Dict:
        """
        ç´” LLM å°è©±ï¼šä¸ä½¿ç”¨ RAG æª¢ç´¢ï¼Œç›´æ¥ç”¨ LLM å›ç­”ã€‚
        é©ç”¨æ–¼ä¸€èˆ¬æ€§å•é¡Œã€é–’èŠç­‰ä¸éœ€è¦æ–‡æª”çŸ¥è­˜çš„å ´æ™¯ã€‚
        
        Args:
            question: ç”¨æˆ¶å•é¡Œ
            model_id: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ IDï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é è¨­æ¨¡å‹
        """
        try:
            # ç²å– LLM å¯¦ä¾‹
            llm = self._get_llm(model_id)
            
            # ç›´æ¥ç”¨ LLM å›ç­”ï¼Œä¸æª¢ç´¢æ–‡æª”
            message = HumanMessage(content=question)
            answer = llm.invoke([message]).content
            
            logger.info(f"âœ… ç´” LLM å°è©±å®Œæˆ (model: {model_id or 'default'})")
            return {
                "question": question,
                "answer": answer,
                "sources": [],  # ç´”å°è©±æ¨¡å¼æ²’æœ‰ä¾†æºæ–‡æª”
                "model": model_id or self.settings.LLM_MODEL_NAME,
                "timestamp": datetime.now().isoformat()
            }
        
        except Exception as e:
            logger.error(f"åŸ·è¡Œå°è©±æ™‚å‡ºéŒ¯: {e}", exc_info=True)
            raise RuntimeError(f"å°è©±å¤±æ•—: {str(e)}")
    
    def _get_llm(self, model_id: Optional[str] = None):
        """
        ç²å– LLM å¯¦ä¾‹
        
        Args:
            model_id: æ¨¡å‹ IDï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é è¨­æ¨¡å‹
        
        Returns:
            LLM å¯¦ä¾‹
        """
        if model_id is None:
            if self.default_llm is None:
                raise RuntimeError("é è¨­ LLM æœªåˆå§‹åŒ–")
            return self.default_llm
        
        # å‹•æ…‹å‰µå»º LLM
        try:
            return self.model_manager.create_llm(model_id, self.settings.LLM_TEMPERATURE)
        except Exception as e:
            logger.error(f"å‰µå»º LLM å¤±æ•— ({model_id}): {e}")
            # å›é€€åˆ°é è¨­ LLM
            logger.warning(f"å›é€€åˆ°é è¨­ LLM")
            if self.default_llm is None:
                raise RuntimeError("é è¨­ LLM æœªåˆå§‹åŒ–ä¸”ç„¡æ³•å‰µå»ºæŒ‡å®šæ¨¡å‹")
            return self.default_llm