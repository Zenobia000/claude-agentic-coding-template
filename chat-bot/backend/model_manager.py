"""
æ¨¡å‹ç®¡ç†å™¨
è² è²¬åŠ è¼‰æ¨¡å‹é…ç½®ã€ç”Ÿæˆæ¨¡å‹åˆ—è¡¨ã€å‰µå»º LLM å¯¦ä¾‹
"""
import json
import logging
import os
from typing import Dict, List, Optional, Any
from pathlib import Path

logger = logging.getLogger(__name__)

class ModelManager:
    def __init__(self, config_path: str = "models_config.json"):
        self.config_path = config_path
        self.models_config = self._load_config()
        self.available_models = self._generate_available_models()
        
    def _load_config(self) -> Dict:
        """è¼‰å…¥æ¨¡å‹é…ç½®æ–‡ä»¶"""
        try:
            config_file = Path(__file__).parent / self.config_path
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹é…ç½®: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ¨¡å‹é…ç½®å¤±æ•—: {e}")
            return {}
    
    def _generate_available_models(self) -> List[Dict]:
        """
        ç”Ÿæˆå¯ç”¨æ¨¡å‹åˆ—è¡¨
        å°æ–¼ chat_multimodal å’Œ reasoning é¡å‹ï¼Œè‡ªå‹•ç”Ÿæˆå…©å€‹ç‰ˆæœ¬ï¼š
        1. åŸå§‹æ¨¡å‹ (ç´” LLM)
        2. {model_id}-rag (RAG æ¨¡å¼)
        """
        models = []
        
        for provider, categories in self.models_config.items():
            # Ollama ä¸éœ€è¦ API Keyï¼Œæª¢æŸ¥ BASE_URL
            if provider == "ollama":
                ollama_url = os.getenv("OLLAMA_BASE_URL")
                if not ollama_url:
                    logger.warning(f"âš ï¸ OLLAMA_BASE_URL æœªè¨­å®šï¼Œè·³é ollama æ¨¡å‹")
                    continue
                # æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨
                try:
                    import requests
                    response = requests.get(f"{ollama_url}/api/tags", timeout=2)
                    if response.status_code != 200:
                        logger.warning(f"âš ï¸ Ollama æœå‹™æœªå°±ç·’ï¼Œè·³é ollama æ¨¡å‹")
                        continue
                except Exception as e:
                    logger.warning(f"âš ï¸ ç„¡æ³•é€£æ¥ Ollama æœå‹™: {e}ï¼Œè·³é ollama æ¨¡å‹")
                    continue
            else:
                # å…¶ä»–æä¾›å•†æª¢æŸ¥ API Key
                api_key_name = f"{provider.upper()}_API_KEY"
                if not os.getenv(api_key_name):
                    logger.warning(f"âš ï¸ {api_key_name} æœªè¨­å®šï¼Œè·³é {provider} æ¨¡å‹")
                    continue
            
            for category, model_list in categories.items():
                # åªå°å°è©±é¡å‹çš„æ¨¡å‹ç”Ÿæˆ RAG ç‰ˆæœ¬
                generate_rag_version = category in ["chat_multimodal", "reasoning"]
                
                for model in model_list:
                    model_id = model["id"]
                    tags = model.get("tags", [])
                    
                    # åŸå§‹æ¨¡å‹ (ç´” LLM)
                    models.append({
                        "id": model_id,
                        "object": "model",
                        "created": 1234567890,
                        "owned_by": provider,
                        "provider": provider,
                        "category": category,
                        "label": model.get("label", model_id),
                        "tags": tags,
                        "rag_enabled": False
                    })
                    
                    # RAG ç‰ˆæœ¬
                    if generate_rag_version:
                        rag_model_id = f"{model_id}-rag"
                        models.append({
                            "id": rag_model_id,
                            "object": "model",
                            "created": 1234567890,
                            "owned_by": provider,
                            "provider": provider,
                            "category": category,
                            "label": f"{model.get('label', model_id)}_rag",
                            "tags": tags + ["rag"],
                            "rag_enabled": True,
                            "base_model": model_id
                        })
        
        logger.info(f"âœ… ç”Ÿæˆ {len(models)} å€‹å¯ç”¨æ¨¡å‹")
        return models
    
    def get_models_list(self) -> List[Dict]:
        """
        è¿”å›ç”¨æˆ¶å¯é¸æ“‡çš„æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨æ–¼ /v1/models APIï¼‰
        
        æ’é™¤ä»¥ä¸‹é¡å‹ï¼š
        - embedding: RAG åº•å±¤æŠ€è¡“ï¼Œç”¨æˆ¶ç„¡éœ€é¸æ“‡
        - audio_realtime: èªéŸ³åŠŸèƒ½æš«ä¸é–‹æ”¾
        """
        # æ’é™¤çš„é¡å‹
        excluded_categories = ["embedding", "audio_realtime"]
        
        return [
            {
                "id": model["id"],
                "object": model["object"],
                "created": model["created"],
                "owned_by": model["owned_by"]
            }
            for model in self.available_models
            if model.get("category") not in excluded_categories
        ]
    
    def get_model_info(self, model_id: str) -> Optional[Dict]:
        """ç²å–ç‰¹å®šæ¨¡å‹çš„è©³ç´°ä¿¡æ¯"""
        for model in self.available_models:
            if model["id"] == model_id:
                return model
        return None
    
    def is_rag_model(self, model_id: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚º RAG æ¨¡å‹"""
        model_info = self.get_model_info(model_id)
        if model_info:
            return model_info.get("rag_enabled", False)
        # å‘å¾Œå…¼å®¹ï¼šæª¢æŸ¥æ˜¯å¦ä»¥ -rag çµå°¾
        return model_id.endswith("-rag")
    
    def get_base_model_id(self, model_id: str) -> str:
        """
        ç²å–åŸºç¤æ¨¡å‹ IDï¼ˆå»é™¤ -rag å¾Œç¶´ï¼‰
        ç”¨æ–¼å¯¦éš›èª¿ç”¨ LLM API
        """
        model_info = self.get_model_info(model_id)
        if model_info and model_info.get("rag_enabled"):
            return model_info.get("base_model", model_id.replace("-rag", ""))
        return model_id
    
    def get_provider(self, model_id: str) -> Optional[str]:
        """ç²å–æ¨¡å‹çš„æä¾›å•†"""
        model_info = self.get_model_info(model_id)
        if model_info:
            return model_info.get("provider")
        return None
    
    def create_llm(self, model_id: str, temperature: float = 0.0):
        """
        æ ¹æ“šæ¨¡å‹ ID å‰µå»ºå°æ‡‰çš„ LLM å¯¦ä¾‹
        """
        provider = self.get_provider(model_id)
        base_model_id = self.get_base_model_id(model_id)
        
        if not provider:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹: {model_id}")
        
        try:
            if provider == "openai":
                from langchain_openai import ChatOpenAI
                api_key = os.getenv("OPENAI_API_KEY")
                return ChatOpenAI(
                    model_name=base_model_id,
                    temperature=temperature,
                    api_key=api_key
                )
            
            elif provider == "anthropic":
                from langchain_anthropic import ChatAnthropic
                api_key = os.getenv("ANTHROPIC_API_KEY")
                return ChatAnthropic(
                    model=base_model_id,
                    temperature=temperature,
                    api_key=api_key
                )
            
            elif provider == "google":
                from langchain_google_genai import ChatGoogleGenerativeAI
                api_key = os.getenv("GOOGLE_API_KEY")
                return ChatGoogleGenerativeAI(
                    model=base_model_id,
                    temperature=temperature,
                    google_api_key=api_key
                )
            
            elif provider == "ollama":
                from langchain_ollama import ChatOllama
                ollama_url = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
                return ChatOllama(
                    model=base_model_id,
                    temperature=temperature,
                    base_url=ollama_url
                )
            
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æä¾›å•†: {provider}")
                
        except ImportError as e:
            logger.error(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾è³´: {e}")
            logger.info(f"ğŸ’¡ è«‹å®‰è£å°æ‡‰çš„å¥—ä»¶ï¼Œä¾‹å¦‚: pip install langchain-{provider}")
            raise
        except Exception as e:
            logger.error(f"âŒ å‰µå»º LLM å¯¦ä¾‹å¤±æ•—: {e}")
            raise
    
    def get_models_by_tag(self, tag: str) -> List[Dict]:
        """æ ¹æ“šæ¨™ç±¤ç¯©é¸æ¨¡å‹"""
        return [
            model for model in self.available_models
            if tag in model.get("tags", [])
        ]
    
    def get_models_by_provider(self, provider: str) -> List[Dict]:
        """æ ¹æ“šæä¾›å•†ç¯©é¸æ¨¡å‹"""
        return [
            model for model in self.available_models
            if model.get("provider") == provider
        ]
    
    def get_chat_models(self) -> List[Dict]:
        """ç²å–æ‰€æœ‰å°è©±æ¨¡å‹"""
        return [
            model for model in self.available_models
            if model.get("category") in ["chat_multimodal", "reasoning"]
        ]
    
    def get_embedding_models(self) -> List[Dict]:
        """
        ç²å–æ‰€æœ‰ embedding æ¨¡å‹
        
        æ³¨æ„ï¼šé€™äº›æ¨¡å‹ä¸æœƒé¡¯ç¤ºåœ¨ /v1/models API ä¸­ï¼Œ
        ä½†ä»ç„¶å¯ä»¥é€šéæ­¤æ–¹æ³•ç²å–ï¼Œç”¨æ–¼ RAG ç³»çµ±å…§éƒ¨ä½¿ç”¨
        """
        return [
            model for model in self.available_models
            if model.get("category") == "embedding"
        ]

# å‰µå»ºå…¨å±€å¯¦ä¾‹
model_manager = ModelManager()

